---
title: "results_summary"
author: "Gabriel Strain"
date: "2023-06-21"
output:
  html_document: default
  pdf_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, warning = FALSE, message = FALSE, comment = FALSE)
```

```{r}
# Loading packages
library(papaja) 
library(tidyverse) 
library(ordinal) 
library(patchwork)
library(magick) 
library(markdown)
library(shiny)
library(knitr)
library(tinytex)
library(scales) 
library(buildmer) 
library(lme4)
library(broom)
library(insight)
#library(kableExtra)
library(effectsize)
library(qwraps2)
library(conflicted)
library(Hmisc)
library(ggdist)
library(emmeans)

# fix function conflicts now 

conflicts_prefer(dplyr::select(), dplyr::filter(), dplyr::lead(), lme4::lmer())

set.seed(54621) # seed for random number generation
#tlmgr_install('collection-fontsrecommended') # additional font collection required for Docker
```

```{r eval-models, include=FALSE}
# in this script, models are cached. If eval_models <- FALSE, script will load
# cached models. Set eval_models <- TRUE to rebuild models from scratch

eval_models <- TRUE

if (eval_models == FALSE){
  lazyload_cache_dir('results_summary/html')
}
```

```{r load-data, include=FALSE}

trusty_viz_data_anon <- read_csv("data/trust_data.csv")
```

```{r wrangle, include=FALSE}

wrangle <- function(anon_file) {
  
  literacy <- anon_file %>%
    filter(!is.na(q1_slider.response)) %>%
    rowwise() %>%
    mutate(literacy = sum(c(q1_slider.response, 
                            q2_slider.response, 
                            q3_slider.response, 
                            q4_slider.response, 
                            q5_slider.response))) %>%
    select(participant,
           literacy)
  
# extract and process monitor and dot pitch information
# we assume standard 16:9 aspect ratio for monitors
  
monitor_information <- anon_file %>%
  mutate(height = dplyr::lead(height)) %>%
  mutate(res_height = res_width*0.5625,
         width = height*0.5625,
         dot_pitch = ((sqrt(height^2 + width^2))/(sqrt(res_height^2 + res_width^2))) * 25.4) %>%
         select(c("dot_pitch", "participant", "res_width")) %>%
  na.omit()
    
  
# extract demographic information
# link slider response numbers to gender categories
  
  demographics <- anon_file %>%
    filter(!is.na(gender_slider.response)) %>%
    mutate(gender_slider.response = recode(gender_slider.response,
                                         `1` = "F",
                                         `2` = "M",
                                         `3` = "NB")) %>%
  select(matches(c("participant",
                          "age_textbox.text",
                          "gender_slider.response")))

# split images column into dataset, plot type, granularity, and correct answer columns 

anon_file <- anon_file %>%
  separate(images, c("dataset", "plot_type", "granularity", "place_holder1", "low_correct",
                     "place_holder2", "high_correct"), sep = "_") %>%
  mutate(high_correct = str_replace(high_correct, pattern = ".png", replacement = ""))

# select relevant columns
# select only experimental items
# add literacy data
# change data types where appropriate
# output this file with suffix 'tidy'

anon_file %>%
  select(c("participant",
           "dataset", # this is the dataset used to generate the plot, there are 45 sets
           "plot_type", # either bar or line
           "granularity", # the number of data points on the plot, 10 for low gran and 40 for high
           "low_correct", # correct answer for the "type in the value of the lowest data point" question
           "high_correct", # correct answer for the "type in the value of the highest data point" question
           "unique_item_no", # n = 180 unique plots
           "session",
           "trials.thisN", # needed to figure out session half
           "textbox_number_component.text", # answer to first question
           "number_keypress.rt", # reaction time for first question
           "trust_slider.response", # trust rating
           "trust_key.rt")) %>% # reaction time for trust rating
  mutate(half = case_when(
    trials.thisN < 93 ~ "First",
    trials.thisN > 92 ~ "Second" )) %>%
  mutate(total_RT = number_keypress.rt + trust_key.rt) %>% # create total reaction time column
  filter(unique_item_no < 181) %>%
  inner_join(literacy, by = "participant") %>%
  inner_join(demographics, by = "participant") %>%
  inner_join(monitor_information, by = "participant") %>%
  select(-c("__participant")) %>%
  mutate(across(matches(c("plot_type", "granularity")), as_factor)) %>%
  mutate(across(c("trust_slider.response"), as.ordered)) %>%
  assign(paste0(unique(anon_file$expName), "_tidy"),
           value = ., envir = .GlobalEnv)
}

# use wrangle function on anonmyised data file 

wrangle(trusty_viz_data_anon)

# remove anon df from environment

rm(trusty_viz_data_anon)

# extract age data

age <- distinct(trusty_viz_tidy, participant,
                .keep_all = TRUE) %>%
  summarise(mean = mean(age_textbox.text, na.rm = TRUE),
            sd = sd(age_textbox.text, na.rm = TRUE)) 

# extract gender data

gender <- distinct(trusty_viz_tidy, participant,
                      .keep_all = TRUE) %>%
  group_by(gender_slider.response) %>%
  summarise(perc = n()/nrow(.)*100) %>%
  pivot_wider(names_from = gender_slider.response, values_from = perc)

# extract literacy data

literacy <- distinct(trusty_viz_tidy, participant,
                        .keep_all = TRUE) %>%
  summarise(mean = mean(literacy), sd = sd(literacy))

```

```{r comparison, include=FALSE}

comparison <- function(model) {
  
  parens <- function(x) paste0("(",x,")")
  onlyBars <- function(form) reformulate(sapply(findbars(form),
                                              function(x)  parens(deparse(x))),
                                       response=".")
  onlyBars(formula(model))
  cmpr_model <- update(model,onlyBars(formula(model)))
  
  return(cmpr_model)
  
}
```

```{r anova-results-lm, include=FALSE}

anova_results_lm <- function(model, cmpr_model) {
  
  model_name <- deparse(substitute(model))
  
  if (class(model) == "buildmer") model <- model@model
  if (class(cmpr_model) == "buildmer") cmpr_model <- cmpr_model@model
  
  anova_output <- anova(model, cmpr_model)
  
  assign(paste0(model_name, ".Chisq"),
         anova_output$Chisq[2],
         envir = .GlobalEnv)
  assign(paste0(model_name, ".df"),
         anova_output$Df[2],
         envir = .GlobalEnv)
  assign(paste0(model_name, ".p"),
         anova_output$`Pr(>Chisq)`[2],
         envir = .GlobalEnv)
  
}
```

```{r anova-results-ord, include=FALSE}
# this function takes two nested models, runs an anova, and the outputs the Likelihood Ratio Statistic, degrees of freedom, and p value to the global environment
anova_results_ord <- function(model, cmpr_model) {
  
  # first argument 
  model_name <- deparse(substitute(model))
  
  if (class(model) == "buildmer") model <- model@model
  if (class(cmpr_model) == "buildmer") cmpr_model <- cmpr_model@model
      
  anova_output <- ordinal:::anova.clm(model, cmpr_model)
  # use of ordinal:::anova.clm based on https://github.com/runehaubo/ordinal/issues/38  
  
  assign(paste0(model_name, ".LR"),
         anova_output$LR.stat[2],
         envir = .GlobalEnv)
  assign(paste0(model_name, ".df"),
         anova_output$df[2],
         envir = .GlobalEnv)
  assign(paste0(model_name, ".p"),
         anova_output$`Pr(>Chisq)`[2],
         envir = .GlobalEnv)
}
```

```{r contrasts-extract, include=FALSE}

contrasts_extract <- function(model) {
  
  model_name <- deparse(substitute(model))
  
  if (class(model) == "buildmer") model <- model@model
  
  EMMs <- emmeans(model, pairwise ~ size)
  
  params <- as.data.frame(EMMs[2]) %>%
                            rename_with(str_replace,
                                        pattern = "contrasts.", replacement = "",
                                        matches("contrasts")) %>%
                            rename_with(str_to_title, !starts_with("p")) %>%
                            select(c("Contrast", "Z.ratio", "p.value"))
  
  return(params)
  
}
```

```{r get-lm-effect-sizes, include=FALSE}

get_effects_sizes <- function(model, d) {
  
  effect_sizes <- lme.dscore(model, data = d, type = "lme4")
  
  effects_df <- as.data.frame(effect_sizes[3])
  
  return(effects_df)
}
```

```{r summary-extract}
summary_extract <- function(model, key_term) {
  
  params <- c("statistic", "p.value", "estimate")

  model_name <- deparse(substitute(model))
  
  if (class(model) == "buildmer") model <- model@model
  
  # get the row for the chosen fixed effect term
  one_row <- tidy(model) %>% filter(term == key_term)

    get_cols <- function(param) {

    assign(value = one_row %>% pull(param),
           envir = .GlobalEnv,
           paste0(model_name, ".", param))
    }

    lapply(params, get_cols)
    
    
    assign(value = confint(model)[key_term,],
           envir = .GlobalEnv,
           paste0(model_name, ".CI"))

}
```

# H1: Granularity does not predict RT

```{r gran-rt-model, cache=eval_models, cache.comments=FALSE, eval=eval_models, include=FALSE, cache.path="results_summary/html/"}

gran_RT_model <- buildmer(total_RT ~ granularity +
                            (1 + granularity | participant) +
                            (1 + granularity | dataset), 
                          data = trusty_viz_tidy)

gran_RT_model <- gran_RT_model@model

gran_RT_comparison <- comparison(gran_RT_model)
```

```{r gran-RT-anova, eval=TRUE}

anova_results_lm(gran_RT_model, gran_RT_comparison)
```

All analyses were built using R (version `r paste0(R.version$major, ".", R.version$minor)`.
Models were built using the **buildmer** (reference) and **lme4** packages, with plot granularity being set as the predictor
for total reaction time. A likelihood ratio test revealed that the model including
plot granularity as a predictor failed to explain significantly more variance than
a null model ($\chi^2$(`r in_paren(gran_RT_model.df)`) = `r printnum(gran_RT_model.Chisq)`,
*p* `r printp(gran_RT_model.p, add_equals = TRUE)`). This model has a random intercepts 
and slopes for participants.

# H2: Granularity can predict higher trust ratings

```{r gran-trust-model, cache=eval_models, cache.comments=FALSE, eval=eval_models, include=FALSE, cache.path="results_summary/html/"}

gran_trust <- buildclmm(trust_slider.response ~ granularity +
                          (1 + granularity | participant) +
                          (1 + granularity | dataset),
                        data = trusty_viz_tidy)

gran_trust_model <- gran_trust@model

gran_trust_comparison <- comparison(gran_trust_model)
```

```{r gran-trust-anova, eval=TRUE}
anova_results_ord(gran_trust_model, gran_trust_comparison)
```

Analyses were built using R (version `r paste0(R.version$major, ".", R.version$minor)`.
Ordinal model was built using the **buildmer** and **ordinal** packages, with plot granularity being set as the predictor
for trustworthiness rating. A likelihood ratio test revealed that the model including
plot granularity as a predictor explained significantly more variance than
a null model ($\chi^2$(`r in_paren(gran_trust_model.df)`) = `r printnum(gran_trust_model.LR)`,
*p* `r printp(gran_trust_model.p, add_equals = TRUE)`). Higher granularity plots were
rated as being more trustworthy. This model has no random intercepts or slopes.

```{r gran_trust_emm-plot, echo=FALSE}
# extract estimated marginal means for E1 chance (magnitude) ratings
gran_trust_emm <- emmeans(gran_trust_model, ~ granularity) %>% as_tibble()

# generate plot of estimated marginal means for trust ratings in the two granularity conditions

gran_trust_emm %>%
  as_tibble() %>%
  mutate_at(vars("emmean":"asymp.UCL"), as.numeric) %>%
  ggplot(aes(x = granularity, y = emmean, group = 1)) +
  geom_linerange(aes(ymin = asymp.LCL, ymax = asymp.UCL),
                 position = position_dodge(width = 0.1),
                 size = 3, alpha = 0.5) +
  geom_point(position = position_dodge(width = 0.1), size = 3) +
  geom_line(position = position_dodge(width = 0.1),
            size = 2) +
  #lims(y = c(-1.8, 2)) +
  labs(y = "Estimated\nMarginal Mean",
       x = "Granularity",
       title = "Ratings of Trustworthiness") +
  scale_x_discrete(labels = c('Low\nGranularity','High\nGranularity'),
                   limits = c("LG", "HG")) +
  theme_minimal(base_size = 18) +
  theme(legend.position = "none",
        panel.grid = element_blank(),
        #axis.text.y = element_blank(),
        plot.title = element_text(size=18, hjust = 0.5))
```

```{r pairwise-comp-gran-rt}
emmeans(gran_trust_model, ~ granularity)
```

# H3: Weak correlation between granularity and trustworthiness

```{r, echo=TRUE}
rcorr(trusty_viz_tidy$trust_slider.response, trusty_viz_tidy$granularity, type = "pearson")
```

# H4: Granularity and reaction time predict trustworthiness, but there is no interaction.


```{r gran-trust-RT-model, eval=eval_models, cache=eval_models, cache.comments=FALSE, cache.path="results_summary/html/", include=FALSE}

gran_trust_RT <- buildclmm(trust_slider.response ~ granularity*total_RT +
                          (1 + granularity * total_RT | participant) +
                          (1 + granularity * total_RT | dataset),
                        data = trusty_viz_tidy)

gran_trust_RT_model <- gran_trust_RT@model

gran_trust_RT_comparison <- clm(trust_slider.response ~ 1, data = trusty_viz_tidy)
```

```{r gran-trustRT-anova}
anova_results_ord(gran_trust_RT_model, gran_trust_comparison)
```

Analysis were built using R (version `r paste0(R.version$major, ".", R.version$minor)`.
Ordinal model was built using the **buildmer** and **ordinal** packages, with plot granularity and total reaction time
being set as the predictors for trustworthiness rating. A likelihood ratio test revealed that the model including
plot granularity and total RT as predictors explained significantly more variance than
a null model ($\chi^2$(`r in_paren(gran_trust_RT_model.df)`) = `r printnum(gran_trust_RT_model.LR)`,
*p* `r printp(gran_trust_RT_model.p, add_equals = TRUE)`). Higher granularity plots were
rated as being more trustworthy, and on average peope were faster to respond
to plots that they subsequently rated as more trustworthy. This model has no random intercepts or
and slopes for items or participants.

```{r dot-plot-RT}



trusty_viz_tidy %>%
  drop_na(trust_slider.response) %>%
  group_by(trust_slider.response) %>%
  summarise(mean_total_RT = mean(total_RT),
            sem = (sd(total_RT)/sqrt(150))) %>%
              ggplot(aes(x = trust_slider.response, y = mean_total_RT, group = 1)) +
              geom_point(size = 1.2) +
              geom_line(size = 0.5) +
              geom_errorbar(mapping = aes(ymin = mean_total_RT - sem, ymax = mean_total_RT + sem),
                            width = 0.07, size = 0.03) +
                theme_ggdist() +
  labs(x = "Trust Rating",
       y = "Mean Reaction Time (ms)",
       title = "Distribution of Reaction Times by Trust Rating")
  

```

```{r}
#| label: bar-plot

trusty_viz_tidy %>%
  filter(!is.na(trust_slider.response)) %>%
  group_by(trust_slider.response, granularity) %>%
    dplyr::count(trust_slider.response) %>%
  ggplot(aes(x = trust_slider.response, y = n, fill = granularity)) +
  geom_bar(stat = "identity", position = "dodge") +
  theme_ggdist() +
  labs(x = "Trust Rating",
       y = "Count",
       title = "Histogram of Trust Ratings by Granularity")
```

# Graph Literacy

```{r graph-lit, cache=eval_models, cache.comments=FALSE, eval=eval_models, include=FALSE, cache.path="results_summary/html/"}
gran_trust_RT_lit <- clm(add.terms(formula(gran_trust_RT_model), "literacy"),
                         data = trusty_viz_tidy)

lit_comp <- gran_trust_RT_model
```

```{r lit-comparison, eval=TRUE, echo=FALSE, message=FALSE}
anova_results_ord(lit_comp, gran_trust_RT_lit)
```

```{r lit-summary}
summary_extract(gran_trust_RT_lit, "literacy")
```

We also generate an additional model to test whether the results we found could
be explained by differences in graph literacy. This model is identical to the 
experimental model, but includes graph literacy as a fixed effect.
We found a significant difference between the original model and the one
including graph literacy as a fixed effect ($\chi^2$(`r in_paren(lit_comp.df)`)
`r printnum(lit_comp.LR)`, *p* `r printp(lit_comp.p, add_equals = TRUE)`). These 
results suggest that the effect we found may have been driven by differences in
graph literacy between participants. A 1 point increase in graph literacy score
corresponded to a `r printnum(gran_trust_RT_lit.estimate)` increase in trust score,
(z = `r printnum(gran_trust_RT_lit.statistic)`, *p* `r printp(gran_trust_RT_lit.p.value, add_equals = TRUE)`, `r print_confint(gran_trust_RT_lit.CI)`).




































